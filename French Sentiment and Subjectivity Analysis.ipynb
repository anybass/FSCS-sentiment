{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75404025",
   "metadata": {},
   "source": [
    "# FRENCH\n",
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U textblob-fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "fr_df_factor = pd.read_pickle('data/fr_df_factor.pkl')\n",
    "fr_df_factor=fr_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "class_dict=defaultdict(int)\n",
    "label_dict=defaultdict(int)\n",
    "subjectivity_dict=defaultdict(int)\n",
    "sub_label_dict=defaultdict(int)\n",
    "sentiment=[]\n",
    "subjectivity=[]\n",
    "sent_stats=[]\n",
    "subj_stats=[]\n",
    "for i in tqdm(range(len(fr_df_factor)), position=0, leave=True):\n",
    "    \n",
    "    classes = tb(fr_df_factor.text[i])\n",
    "    sent=classes.polarity\n",
    "    sent_stats.append(sent)\n",
    "    # earlier, we classified it more detailed. For better comparison, we have no slight or strong levels anymore, \n",
    "    # just negative, positive and neutral\n",
    "    '''if sent>=0.5:\n",
    "        class_dict[\"strong_positive\"]+=1\n",
    "        label_dict[str(\"strong_positive_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"strong_positive\")\n",
    "    elif sent<=-0.5:\n",
    "        class_dict[\"strong_negative\"]+=1\n",
    "        label_dict[str(\"strong_negative_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"strong_negative\")'''\n",
    "    if sent<=-0.6:\n",
    "        class_dict[\"negative\"]+=1\n",
    "        label_dict[str(\"negative_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"negative\")\n",
    "\n",
    "    elif sent>=0.6:\n",
    "        class_dict[\"positive\"]+=1\n",
    "        label_dict[str(\"positive_\"+str(fr_df_factor.label[i]))]+=1        \n",
    "        sentiment.append(\"positive\")\n",
    "\n",
    "        '''elif sent>-0.3 and sent<=-0.1:\n",
    "        class_dict[\"slightly_negative\"]+=1\n",
    "        label_dict[str(\"slightly_negative_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"slightly_negative\")\n",
    "\n",
    "    elif sent<0.3 and sent>=0.1:\n",
    "        class_dict[\"slightly_positive\"]+=1\n",
    "        label_dict[str(\"slightly_positive_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"slightly_positive\")'''\n",
    "\n",
    "    else:\n",
    "        class_dict[\"neutral\"]+=1 \n",
    "        label_dict[str(\"neutral_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        sentiment.append(\"neutral\")\n",
    "\n",
    "    sub=classes.subjectivity\n",
    "    subj_stats.append(sub)\n",
    "\n",
    "    if sub>=0.9:\n",
    "\n",
    "        subjectivity_dict[\"subjective\"]+=1\n",
    "        sub_label_dict[str(\"subjective_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        subjectivity.append(\"subjective\")\n",
    " \n",
    "    else:\n",
    "        subjectivity_dict[\"not_subjective\"]+=1 \n",
    "        sub_label_dict[str(\"not_subjective_\"+str(fr_df_factor.label[i]))]+=1\n",
    "        subjectivity.append(\"not_subjective\")\n",
    "    \n",
    "fr_df_factor[\"sentiment\"]=sentiment\n",
    "fr_df_factor[\"subjectivity\"]=subjectivity\n",
    "fr_df_factor[\"sentiment_score\"]=sent_stats\n",
    "fr_df_factor[\"subjectivity_score\"]=subj_stats\n",
    "fr_df_factor.to_pickle('data/fr_df_sentiments.pkl')\n",
    "print(class_dict.items())\n",
    "print(label_dict.items())\n",
    "print(subjectivity_dict.items())\n",
    "print(sub_label_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_stats)\n",
    "print(min(sent_stats))\n",
    "print(max(sent_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=sent_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=sent_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.35,0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ead6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subj_stats)\n",
    "print(min(subj_stats))\n",
    "print(max(subj_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=subj_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=subj_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([0,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.etree import cElementTree\n",
    "def load(path):\n",
    "        \"\"\" Loads the XML-file (with sentiment annotations) from the given path.\n",
    "            By default, Sentiment.path is lazily loaded.\n",
    "        \"\"\"\n",
    "        # <word form=\"great\" wordnet_id=\"a-01123879\" pos=\"JJ\" polarity=\"1.0\" subjectivity=\"1.0\" intensity=\"1.0\" />\n",
    "        # <word form=\"damnmit\" polarity=\"-0.75\" subjectivity=\"1.0\" label=\"profanity\" />\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        words, synsets, labels = {}, {}, {}\n",
    "        xml = cElementTree.parse(path)\n",
    "        xml = xml.getroot()\n",
    "        lex={}\n",
    "        for w in xml.findall(\"word\"):\n",
    "            \n",
    "                w, s  = (\n",
    "                    w.attrib.get(\"form\"),\n",
    "                    w.attrib.get(\"subjectivity\", 0.0),\n",
    "                    \n",
    "                )\n",
    "                if float(s)>0.0:\n",
    "                    lex[w]=s\n",
    "        \n",
    "        return lex     \n",
    "lookup=load(\"data/fr-sentiment.xml\")\n",
    "print(len(list(lookup.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "textblob_df=pd.read_pickle(\"data/fr_df_sentiments.pkl\")\n",
    "subj_df=textblob_df[(textblob_df[\"subjectivity_score\"]>=1.0)]\n",
    "subj_df.reset_index(drop=True, inplace=True)\n",
    "print(len(subj_df))\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "subjsent=[]\n",
    "for i in range(len(subj_df)):\n",
    "    sentences=sent_tokenize(subj_df.loc[i].text, language=\"french\")\n",
    "    \n",
    "    for s in sentences:\n",
    "        token=word_tokenize(s, language=\"french\")\n",
    "        \n",
    "        for tok in token:\n",
    "            if tok.lower() in lookup.keys():\n",
    "                  subjsent.append(s)\n",
    "\n",
    "subs=\" \".join(list(subjsent))\n",
    "text = subs\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "d = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "nichtinteressant = [\"1\", \"er\", \"1er\",\"fr\", \"B\", \"ci\",\"X\", \"juin\", \"décembre\", \"août\", \"février\", \"mars\", \"janvier\", \"avril\", \"mai\", \"juillet\", \"octobre\", \"novembre\", \"septembre\", \"après\"]\n",
    "liste_der_unerwuenschten_woerter = stopwords.words('french')\n",
    "liste_der_unerwuenschten_woerter= liste_der_unerwuenschten_woerter+nichtinteressant\n",
    "STOPWORDS.update(liste_der_unerwuenschten_woerter)\n",
    "wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(subjsent[-600:-450])\n",
    "print(len(subjsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afa87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"compte tenu de son âge - 58 ans au moment des faits - susceptible de l'exposer à de grandes difficultés de réinsertion économique, de la courte durée des relations de travail et de la légèreté de la décision prise.\"\n",
    "\n",
    "words=text.split(\" \")\n",
    "for w in words:\n",
    "    if w in lookup.keys():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "textblob_df=pd.read_pickle('data/fr_df_sentiments.pkl')\n",
    "\n",
    "textblob_dict=defaultdict(int)\n",
    "t_list=[]\n",
    "for t in textblob_df[\"subjectivity\"]:\n",
    "    textblob_dict[t]+=1\n",
    "textblob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "textblob_df=pd.read_pickle('data/fr_df_sentiments.pkl')\n",
    "\n",
    "textblob_dict=defaultdict(int)\n",
    "t_list=[]\n",
    "for t in textblob_df[\"sentiment\"]:\n",
    "    textblob_dict[t]+=1\n",
    "textblob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4089753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "fr_df_sentiment=fr_df_factor[(fr_df_factor[\"sentiment\"] == \"positive\")|(fr_df_factor[\"sentiment\"] == \"strong_positive\")|(fr_df_factor[\"sentiment\"] == \"negative\")|(fr_df_factor[\"sentiment\"] == \"strong_negative\")]\n",
    "\n",
    "\n",
    "\n",
    "visual=[]\n",
    "\n",
    "sentences=[]\n",
    "for i in tqdm(range(len(fr_df_sentiment)), position=0, leave=True):\n",
    "    \n",
    "    tokens=fr_df_sentiment.iloc[i][\"text\"].split()\n",
    "    if visual:\n",
    "        sentences.append(visual)\n",
    "    \n",
    "    visual=[]\n",
    "    \n",
    "    subj=fr_df_sentiment.iloc[i][\"subjectivity\"]\n",
    "    if subj==\"strongly_subjective\":\n",
    "        visual.append(str(\"<em><strong>Subjectivity: \"+subj+\"</em></strong>\"))\n",
    "    elif subj==\"subjective\":\n",
    "        visual.append(str(\"<strong>Subjectivity: \"+subj+\"</strong>\"))\n",
    "    elif subj==\"slightly_subjective\":\n",
    "        visual.append(str(\"<em>Subjectivity: \"+subj+\"</em>\"))\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    s=fr_df_sentiment.iloc[i][\"sentiment\"]\n",
    "    if s==\"strong_positive\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:darkgreen;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    elif s==\"strong_negative\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:darkred;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    elif s==\"negative\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    elif s==\"positive\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    elif s==\"slightly_negative\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:salmon;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    elif s==\"slighly_positive\":\n",
    "        visual.append(str(\"<code style=\"+'\"background:yellowgreen;color:white\"'+\"> \"+s+\" </code>\"))\n",
    "    \n",
    "    visual.append(str(\"<strong>Decision: \"+str(fr_df_sentiment.iloc[i][\"label\"]) +\"</strong> \"))\n",
    "    \n",
    "    for t in tokens:\n",
    "        classes = tb(t)\n",
    "        sub=classes.subjectivity\n",
    "        sent=classes.polarity\n",
    "        if sub>=0.5:\n",
    "            tok=str(\"<em><strong>\"+t+\"</em></strong>\")\n",
    "\n",
    "        elif sub<0.5 and sub>=0.3:\n",
    "            tok=str(\"<strong>\"+t+\"</strong>\")\n",
    "        elif sub<0.3 and sub>=0.1:\n",
    "            tok=str(\"<em>\"+t+\"</em>\")\n",
    "        else:\n",
    "            tok=t\n",
    "        \n",
    "        if sent>=0.5:\n",
    "            visual.append(str(\"<code style=\"+'\"background:darkgreen;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "        elif sent<=-0.5:\n",
    "            visual.append(str(\"<code style=\"+'\"background:darkred;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "        elif sent>-0.5 and sent<=-0.3:\n",
    "            visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "        elif sent<0.5 and sent>=0.3:\n",
    "            visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "        elif sent>-0.3 and sent<=-0.1:\n",
    "            visual.append(str(\"<code style=\"+'\"background:salmon;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "        elif sent<0.3 and sent>=0.1:\n",
    "            visual.append(str(\"<code style=\"+'\"background:yellowgreen;color:white\"'+\"> \"+tok+\" </code>\"))\n",
    "\n",
    "        else:\n",
    "            visual.append(\" \"+tok+\" \")\n",
    "\n",
    "import markdown\n",
    "\n",
    "\n",
    "html_export=[]\n",
    "    \n",
    "for s in sentences:\n",
    "    display(Markdown(\" \".join(s)))\n",
    "    html = markdown.markdown(\" \".join(s))\n",
    "    html_export.append(html)\n",
    "\n",
    "with open(\"fr_sentiments.html\",\"w\", encoding=\"UTF-8\") as f:\n",
    "    for ht in html_export:\n",
    "        f.write(ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ea6ff",
   "metadata": {},
   "source": [
    "## DistilCamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a06545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "\n",
    "nlp = pipeline(\n",
    "    task='text-classification',\n",
    "    model=\"cmarkea/distilcamembert-base-sentiment\",\n",
    "    tokenizer=\"cmarkea/distilcamembert-base-sentiment\",max_length=512, truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sp=[]\n",
    "sn=[]\n",
    "neuneu=[]\n",
    "\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "width=512\n",
    "sentiment_neu=[]\n",
    "fr_df_factor = pd.read_pickle('data/fr_df_factor.pkl')\n",
    "fr_df_factor=fr_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(fr_df_factor)), position=0, leave=True):\n",
    "    senti_neu=defaultdict(int)\n",
    "    doc= nlp(textwrap.wrap(fr_df_factor.text[i], width, break_long_words=False))\n",
    "    \n",
    "  \n",
    "    senti_neu[\"positive\"]=0\n",
    "    senti_neu[\"negative\"]=0\n",
    "    senti_neu[\"neutral\"]=0\n",
    " \n",
    "    \n",
    "    for sent in doc:\n",
    "        lab=sent[\"label\"]\n",
    "        if lab==\"5 stars\":\n",
    "            senti_neu[\"positive\"]+=1\n",
    "        elif lab == \"4 stars\":\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "        elif lab == \"1 star\":\n",
    "            senti_neu[\"negative\"]+=1\n",
    "\n",
    "        elif lab == \"2 stars\":\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "\n",
    "        else:\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "     \n",
    "    sentiment_neu.append(max(senti_neu, key=senti_neu.get))\n",
    "   \n",
    "    sn.append(senti_neu[\"negative\"]/len(list(doc)))\n",
    "    sp.append(senti_neu[\"positive\"]/len(list(doc)))\n",
    "    neuneu.append(senti_neu[\"neutral\"]/len(list(doc)))\n",
    "    \n",
    "fr_df_factor[\"sentiment_neu\"]=sentiment_neu\n",
    "\n",
    "fr_df_factor[\"positive_neu\"]=sp\n",
    "fr_df_factor[\"negative_neu\"]=sn\n",
    "fr_df_factor[\"neutral_neu\"]=neuneu\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "fr_df_factor.to_pickle('data/fr_df_sentiments_distilcamembert_small_no_dup_3classes.pkl')\n",
    "fr_df_factor.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52e7f0",
   "metadata": {},
   "source": [
    "## FlauBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e48985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "sp=[]\n",
    "sn=[]\n",
    "neuneu=[]\n",
    "\n",
    "\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flaubert/flaubert_small_cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/flaubert_small_cased_sentiment\")\n",
    "\n",
    "nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, max_length=512, truncation=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "width=512\n",
    "\n",
    "sentiment_neu=[]\n",
    "fr_df_factor = pd.read_pickle('data/fr_df_factor.pkl')\n",
    "fr_df_factor=fr_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(fr_df_factor)), position=0, leave=True):\n",
    "\n",
    "    senti_neu=defaultdict(int)\n",
    "    doc= nlp(textwrap.wrap(fr_df_factor.text[i], width, break_long_words=False))\n",
    "    \n",
    "  \n",
    "    \n",
    "    senti_neu[\"positive\"]=0\n",
    "    senti_neu[\"negative\"]=0\n",
    "    senti_neu[\"neutral\"]=0\n",
    "    \n",
    "    for sent in doc:\n",
    "        lab=sent[\"label\"]\n",
    "        if lab==\"very_positive\":           \n",
    "            senti_neu[\"positive\"]+=1\n",
    "        elif lab == \"positive\":\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "        elif lab == \"very_negative\":\n",
    "            senti_neu[\"negative\"]+=1\n",
    "        elif lab == \"negative\":\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "        else:\n",
    "            senti_neu[\"neutral\"]+=1\n",
    "     \n",
    "    sentiment_neu.append(max(senti_neu, key=senti_neu.get))\n",
    "   \n",
    "    sn.append(senti_neu[\"negative\"]/len(list(doc)))\n",
    "    sp.append(senti_neu[\"positive\"]/len(list(doc)))\n",
    "    neuneu.append(senti_neu[\"neutral\"]/len(list(doc)))\n",
    "    \n",
    "\n",
    "fr_df_factor[\"sentiment_neu\"]=sentiment_neu\n",
    "\n",
    "fr_df_factor[\"positive_neu\"]=sp\n",
    "fr_df_factor[\"negative_neu\"]=sn\n",
    "fr_df_factor[\"neutral_neu\"]=neuneu\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "fr_df_factor.to_pickle('data/fr_df_sentiments_flaubert_small_no_dup_3classes.pkl')\n",
    "fr_df_factor.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb5f05",
   "metadata": {},
   "source": [
    "## VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a16fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment-fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "fr_nlp = spacy.load('fr_core_news_lg')\n",
    "config = {\"punct_chars\": None}\n",
    "fr_nlp.add_pipe(\"sentencizer\", config=config)\n",
    "sentiment=[]\n",
    "\n",
    "n=[]\n",
    "p=[]\n",
    "neu=[]\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "vader_scores=[]\n",
    "sentiment=[]\n",
    "fr_df_factor = pd.read_pickle('data/fr_df_factor.pkl')\n",
    "fr_df_factor=fr_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(len(fr_df_factor)), position=0, leave=True):\n",
    "    senti=defaultdict(int)\n",
    "    senti[\"positive\"]=0\n",
    "    senti[\"negative\"]=0\n",
    "    senti[\"neutral\"]=0\n",
    "    doc=fr_nlp(fr_df_factor.text[i])\n",
    "    for sentence in doc.sents: \n",
    "        vs=SIA.polarity_scores(sentence.text)\n",
    "    \n",
    "        sent=vs[\"compound\"]\n",
    "        vader_scores.append(sent)\n",
    "        if sent>=0.5:\n",
    "            senti[\"positive\"]+=1\n",
    "        elif sent<=-0.5:\n",
    "            senti[\"negative\"]+=1\n",
    "        else:\n",
    "            senti[\"neutral\"]+=1\n",
    "\n",
    "  \n",
    "    final_class=max(senti, key=senti.get)\n",
    "    sentiment.append(final_class)            \n",
    "        \n",
    "    p.append(senti[\"positive\"]/len(list(doc.sents)))\n",
    "    n.append(senti[\"negative\"]/len(list(doc.sents)))\n",
    "    neu.append(senti[\"neutral\"]/len(list(doc.sents)))\n",
    "fr_df_factor[\"vader_sentiment\"]=sentiment\n",
    "fr_df_factor[\"positive\"]=p\n",
    "fr_df_factor[\"negative\"]=n\n",
    "fr_df_factor[\"neutral\"]=neu\n",
    "  \n",
    "\n",
    "fr_df_factor.to_pickle('data/fr_df_sentiments_vader_no_dup_3classes05.pkl')\n",
    "fr_df_factor.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vader_scores)\n",
    "print(min(vader_scores))\n",
    "print(max(vader_scores))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=vader_scores)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=vader_scores,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.3,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe8f95",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "vader_df=pd.read_pickle('data/fr_df_sentiments_vader_no_dup_3classes05.pkl')\n",
    "\n",
    "print(\"vader:\")\n",
    "print(vader_df.groupby('vader_sentiment').count()['id'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee558b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flaubert_df=pd.read_pickle('data/fr_df_sentiments_flaubert_small_no_dup_3classes.pkl')\n",
    "print(\"flaubert: \")\n",
    "print(print(flaubert_df.groupby('sentiment_neu').count()['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2331669",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_df=pd.read_pickle(\"data/fr_df_sentiments_distilcamembert_small_no_dup_3classes.pkl\")\n",
    "print(\"camembert: \")\n",
    "print(cam_df.groupby('sentiment_neu').count()['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textblob_df=pd.read_pickle('data/fr_df_sentiments.pkl')\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(\"Pairwise Cohen Kappa:\")\n",
    "print(str(\"Flaubert and VADER: \"+str(cohen_kappa_score(flaubert_df[\"sentiment_neu\"],vader_df[\"vader_sentiment\"]))))\n",
    "print(str(\"Flaubert and DCamemBERT: \"+str(cohen_kappa_score(flaubert_df[\"sentiment_neu\"],cam_df[\"sentiment_neu\"]))))\n",
    "print(str(\"Flaubert and TextBlob: \"+str(cohen_kappa_score(flaubert_df[\"sentiment_neu\"],textblob_df[\"sentiment\"]))))\n",
    "print(str(\"VADER and DCamemBERT: \"+str(cohen_kappa_score(vader_df[\"vader_sentiment\"],cam_df[\"sentiment_neu\"]))))\n",
    "print(str(\"VADER and TextBlob: \"+str(cohen_kappa_score(vader_df[\"vader_sentiment\"],textblob_df[\"sentiment\"]))))\n",
    "print(str(\"DCamemBERT and TextBlob: \"+str(cohen_kappa_score(cam_df[\"sentiment_neu\"],textblob_df[\"sentiment\"]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9abee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fleiss kappa\n",
    "from nltk import agreement\n",
    "\n",
    "s1=flaubert_df[\"sentiment_neu\"].tolist()\n",
    "s2=cam_df[\"sentiment_neu\"].tolist()\n",
    "s3=vader_df[\"vader_sentiment\"].tolist()\n",
    "s4=textblob_df[\"sentiment\"].tolist()\n",
    "\n",
    "formatted_codes = [[1,i,s1[i]] for i in range(len(s1))] + [[2,i,s2[i]] for i in range(len(s2))]  + [[3,i,s3[i]] for i in range(len(s3))]+ [[4,i,s4[i]] for i in range(len(s4))]\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_codes)\n",
    "print('Fleiss\\'s Kappa:',ratingtask.multi_kappa())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ee681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
