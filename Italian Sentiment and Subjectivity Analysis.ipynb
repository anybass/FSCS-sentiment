{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f2afa2",
   "metadata": {},
   "source": [
    "# ITALIAN\n",
    "\n",
    "## BERT base italian cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ae10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "from collections import defaultdict\n",
    "width=512\n",
    "sentiment=[]\n",
    "n=[]\n",
    "p=[]\n",
    "neu=[]\n",
    "it_df_factor = pd.read_pickle('data/it_df_factor.pkl')\n",
    "it_df_factor=it_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuraly/bert-base-italian-cased-sentiment\")\n",
    "# Load the model, use .cuda() to load it on the GPU\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuraly/bert-base-italian-cased-sentiment\")\n",
    "nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, max_length=512, truncation=True)\n",
    "\n",
    "for i in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "    doc = nlp(textwrap.wrap(it_df_factor.text[i], width, break_long_words=False))\n",
    "    senti=defaultdict(int)\n",
    "  \n",
    "    senti[\"positive\"]=0\n",
    "    senti[\"negative\"]=0\n",
    "    senti[\"neutral\"]=0\n",
    "  \n",
    "    for sent in doc:\n",
    "    \n",
    "        senti[sent[\"label\"]]+=1\n",
    "\n",
    "    sentiment.append(max(senti, key=senti.get))\n",
    "    \n",
    "    p.append(senti[\"positive\"]/len(list(doc)))\n",
    "    n.append(senti[\"negative\"]/len(list(doc)))\n",
    "    neu.append(senti[\"neutral\"]/len(list(doc)))\n",
    "    \n",
    "    \n",
    "it_df_factor[\"sentiment\"]=sentiment\n",
    "it_df_factor[\"positive\"]=p\n",
    "it_df_factor[\"negative\"]=n\n",
    "it_df_factor[\"neutral\"]=neu\n",
    "  \n",
    "\n",
    "it_df_factor.to_pickle('data/it_df_sentiments_bert_whole_len_no_dup.pkl')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_df_factor.groupby('sentiment').count()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_df_factor.loc[(it_df_factor.sentiment==\"negative\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945d676",
   "metadata": {},
   "source": [
    "## Sentita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3906bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "it_nlp=spacy.load(\"it_core_news_sm\")\n",
    "from sentita import calculate_polarity\n",
    "it_df_factor = pd.read_pickle('data/it_df_factor.pkl')\n",
    "it_df_factor=it_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "p=[]\n",
    "n=[]\n",
    "neu=[]\n",
    "sentiment=[]\n",
    "pos_scores=[]\n",
    "neg_scores=[]\n",
    "for i in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "    doc=it_nlp(it_df_factor.text[i])\n",
    "    sentences=[]\n",
    "    for sent in doc:\n",
    "        sentences.append(sent.text)\n",
    "        \n",
    "    \n",
    "    results, polarities = calculate_polarity(sentences)\n",
    "    for posi in polarities:\n",
    "        pos_scores.append(posi[0])\n",
    "        neg_scores.append(posi[1])\n",
    "    pos= [p[0] for p in polarities]\n",
    "    neg=[p[1] for p in polarities]\n",
    "    senti=defaultdict(int)\n",
    "    for s in range(len(pos)):\n",
    "        if pos[s]> neg[s] and pos[s]>0.6:\n",
    "            senti[\"positive\"]+=1\n",
    "        elif neg[s]> pos[s] and neg[s]>0.1:\n",
    "            senti[\"negative\"]+=1\n",
    "        else: \n",
    "            senti[\"neutral\"]+=1\n",
    "\n",
    "            \n",
    "    sentiment.append(max(senti, key=senti.get))\n",
    "    \n",
    "    p.append(senti[\"positive\"]/len(pos))\n",
    "    n.append(senti[\"negative\"]/len(pos))\n",
    "    neu.append(senti[\"neutral\"]/len(pos))\n",
    "    \n",
    "    \n",
    "it_df_factor[\"sentiment\"]=sentiment\n",
    "it_df_factor[\"positive\"]=p\n",
    "it_df_factor[\"negative\"]=n\n",
    "it_df_factor[\"neutral\"]=neu\n",
    "  \n",
    "\n",
    "it_df_factor.to_pickle('data/it_df_sentiments_sentita_whole_len_no_dup.pkl')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_scores)\n",
    "print(min(pos_scores))\n",
    "print(max(pos_scores))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=pos_scores)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=pos_scores,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.15,0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc245b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_scores)\n",
    "print(min(neg_scores))\n",
    "print(max(neg_scores))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=neg_scores)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=neg_scores,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.15,0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5f20a",
   "metadata": {},
   "source": [
    "## NLTK with sentiment lexicon #1 (Ragusalex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af37752",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_WORDS_FILE=\"./italian_sentiment/pos.words.txt\"\n",
    "NEG_WORDS_FILE=\"./italian_sentiment/neg.words.txt\"\n",
    "sentiment_lookup={}\n",
    "for pos_word in open(POS_WORDS_FILE, 'r', encoding=\"latin1\").readlines():\n",
    "    sentiment_lookup[pos_word.strip(\"\\n\")]='positive'\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r', encoding=\"latin1\").readlines():\n",
    "    sentiment_lookup[neg_word.strip(\"\\n\")]='negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in list(sentiment_lookup.items())[-10:]:\n",
    "    print(\"%s: %s\"%(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68508c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "it_df_factor = pd.read_pickle('data/it_df_factor.pkl')\n",
    "it_df_factor=it_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "from collections import defaultdict\n",
    "\n",
    "tokens=[]\n",
    "sentiment=[]\n",
    "sentiment_score_pos=[]\n",
    "sentiment_score_neg=[]\n",
    "sentiment_score_mix=[]\n",
    "sentiment_score=[]\n",
    "for t in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "  sentilist=[]\n",
    "  senti=defaultdict(int)\n",
    "  sentences=sent_tokenize(it_df_factor.loc[t].text, language=\"italian\")\n",
    "  tokensent=[]\n",
    "  for s in sentences:\n",
    "    token=word_tokenize(s, language=\"italian\")\n",
    "    tokensent=tokensent+token\n",
    "    polarityflag=0\n",
    "    posi=[]\n",
    "    nega=[]\n",
    "    for tok in token:\n",
    "        if tok.lower() in sentiment_lookup.keys():\n",
    "          sentilist.append(sentiment_lookup[tok.lower()])\n",
    "          if sentiment_lookup[tok.lower()]==\"positive\":\n",
    "                posi.append(tok.lower())\n",
    "          else:\n",
    "                nega.append(tok.lower())\n",
    "          polarityflag=1\n",
    "        else:\n",
    "          sentilist.append(\"neutral\")\n",
    "    if polarityflag==0:\n",
    "        senti[\"neutral\"]+=1\n",
    "    else:\n",
    "        if len(posi)>len(nega):\n",
    "            senti[\"positive\"]+=1\n",
    "        elif len(nega)>len(posi):\n",
    "            senti[\"negative\"]+=1\n",
    "        else:\n",
    "            senti[\"mixed\"]+=1\n",
    "             \n",
    "  tokens.append(tokensent)\n",
    "  sentiment.append(sentilist)\n",
    "  sentiscore_neg=senti[\"negative\"]\n",
    "  sentiscore_pos=senti[\"positive\"]\n",
    "  sentiscore_mix=senti[\"mixed\"]\n",
    "  aggregated=senti[\"mixed\"]+senti[\"positive\"]-senti[\"negative\"]\n",
    "  sentiscore=aggregated/len(list(sentences))\n",
    "  sentiment_score_neg.append(sentiscore_neg)\n",
    "  sentiment_score_pos.append(sentiscore_pos)\n",
    "  sentiment_score_mix.append(sentiscore_mix)\n",
    "  sentiment_score.append(sentiscore)\n",
    "it_df_factor[\"lookup_sentiment\"]=sentiment\n",
    "it_df_factor[\"tokens\"]=tokens\n",
    "it_df_factor[\"sentiment_score_pos\"]=sentiment_score_pos\n",
    "it_df_factor[\"sentiment_score_neg\"]=sentiment_score_neg\n",
    "it_df_factor[\"sentiment_score_mixed\"]=sentiment_score_mix\n",
    "it_df_factor[\"sentiment_score\"]=sentiment_score\n",
    "it_df_factor.to_pickle('data/it_df_sentiments_lexicon1_no_dup.pkl')\n",
    "\n",
    "print(it_df_factor.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('data/it_df_sentiments_lexicon1_no_dup.pkl')\n",
    "sent_stats=df[\"sentiment_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9676ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_stats)\n",
    "print(min(sent_stats))\n",
    "print(max(sent_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=sent_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=sent_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.025,0.025])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos=0.6\n",
    "threshold_neg=0.7\n",
    "df_pos=sent_stats[(sent_stats>=threshold_pos)]\n",
    "df_neg=sent_stats[(sent_stats<=-threshold_neg)]\n",
    "print(len(df_neg))\n",
    "print(len(df_pos))\n",
    "\n",
    "sentiments=[]\n",
    "for s in sent_stats:\n",
    "    if s<=-threshold_neg:\n",
    "        sentiments.append(\"negative\")\n",
    "    elif s>=threshold_pos:\n",
    "        sentiments.append(\"positive\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "        \n",
    "df[\"lex1_sentiment\"]=sentiments\n",
    "df.to_pickle('data/it_df_sentiments_lexicon1_no_dup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c12f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual=[]\n",
    "\n",
    "sentences=[]\n",
    "for i in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "  if visual:\n",
    "        sentences.append(visual)\n",
    "  visual=[]\n",
    "  for t in range(len(it_df_factor.loc[i,\"tokens\"])):\n",
    "    if it_df_factor.loc[i,\"lookup_sentiment\"][t]==\"negative\":\n",
    "      # adding markup commands to make background for negative sentiment crimson red\n",
    "      visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+it_df_factor.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    elif it_df_factor.loc[i,\"lookup_sentiment\"][t]==\"positive\":\n",
    "      # adding markup commands to make background for positive sentiment mediumseagreen\n",
    "      visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+it_df_factor.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    else:\n",
    "      visual.append(\" \"+it_df_factor.loc[i,\"tokens\"][t]+\" \")\n",
    "\n",
    "import markdown\n",
    "\n",
    "html_export=[]\n",
    "import IPython\n",
    "# helper function to render highlights in this notebook\n",
    "Markdown = lambda string: IPython.display.HTML(markdown.markdown(string))\n",
    "\n",
    "for s in sentences:\n",
    "    # display it in this notebokk\n",
    "    #display(Markdown(\" \".join(s)))\n",
    "    # prepare for export to html file\n",
    "    html = markdown.markdown(\" \".join(s))\n",
    "    html_export.append(html)\n",
    "\n",
    "# export to current working directory\n",
    "with open(\"./it_sentiments.html\",\"w\", encoding=\"UTF-8\") as f:\n",
    "    for ht in html_export:\n",
    "        f.write(ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc180ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score_neg\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55913db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score_pos\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now visualizing only extreme cases\n",
    "extreme_df=it_df_factor[(it_df_factor[\"sentiment_score_neg\"] >= 30)|(it_df_factor[\"sentiment_score_pos\"]>=25)]\n",
    "extreme_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "visual=[]\n",
    "\n",
    "sentences=[]\n",
    "for i in tqdm(range(len(extreme_df[:10])), position=0, leave=True):\n",
    "  if visual:\n",
    "        sentences.append(visual)\n",
    "  visual=[]\n",
    "  for t in range(len(extreme_df.loc[i,\"tokens\"])):\n",
    "    if extreme_df.loc[i,\"lookup_sentiment\"][t]==\"negative\":\n",
    "      # adding markup commands to make background for negative sentiment crimson red\n",
    "      visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+extreme_df.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    elif extreme_df.loc[i,\"lookup_sentiment\"][t]==\"positive\":\n",
    "      # adding markup commands to make background for positive sentiment mediumseagreen\n",
    "      visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+extreme_df.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    else:\n",
    "      visual.append(\" \"+extreme_df.loc[i,\"tokens\"][t]+\" \")\n",
    "\n",
    "import markdown\n",
    "\n",
    "\n",
    "import IPython\n",
    "# helper function to render highlights in this notebook\n",
    "Markdown = lambda string: IPython.display.HTML(markdown.markdown(string))\n",
    "# for instance based inspection use row index and score\n",
    "for s in sentences:\n",
    "    # display it in this notebokk\n",
    "    display(Markdown(\" \".join(s)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97982a44",
   "metadata": {},
   "source": [
    "## NLTK with sentiment lexicon #2 (Porculex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea74679",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_WORDS_FILE=\"./italian_sentiment/posITA.txt\"\n",
    "NEG_WORDS_FILE=\"./italian_sentiment/negITA.txt\"\n",
    "sentiment_lookup={}\n",
    "for pos_word in open(POS_WORDS_FILE, 'r', encoding=\"latin1\").readlines()[17:]:\n",
    "    sentiment_lookup[pos_word.strip(\"\\n\")]='positive'\n",
    "for neg_word in open(NEG_WORDS_FILE, 'r', encoding=\"latin1\").readlines()[17:]:\n",
    "    sentiment_lookup[neg_word.strip(\"\\n\")]='negative'\n",
    "    \n",
    "for k,v in list(sentiment_lookup.items())[:10]:\n",
    "    print(\"%s: %s\"%(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "it_df_factor = pd.read_pickle('data/it_df_factor.pkl')\n",
    "it_df_factor=it_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "from collections import defaultdict\n",
    "\n",
    "tokens=[]\n",
    "sentiment=[]\n",
    "sentiment_score_pos=[]\n",
    "sentiment_score_neg=[]\n",
    "sentiment_score_mix=[]\n",
    "sentiment_score=[]\n",
    "for t in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "  sentilist=[]\n",
    "  senti=defaultdict(int)\n",
    "  sentences=sent_tokenize(it_df_factor.loc[t].text, language=\"italian\")\n",
    "  tokensent=[]\n",
    "  for s in sentences:\n",
    "    token=word_tokenize(s, language=\"italian\")\n",
    "    tokensent=tokensent+token\n",
    "    polarityflag=0\n",
    "    posi=[]\n",
    "    nega=[]\n",
    "    for tok in token:\n",
    "        if tok.lower() in sentiment_lookup.keys():\n",
    "          sentilist.append(sentiment_lookup[tok.lower()])\n",
    "          if sentiment_lookup[tok.lower()]==\"positive\":\n",
    "                posi.append(tok.lower())\n",
    "          else:\n",
    "                nega.append(tok.lower())\n",
    "          polarityflag=1\n",
    "        else:\n",
    "          sentilist.append(\"neutral\")\n",
    "    if polarityflag==0:\n",
    "        senti[\"neutral\"]+=1\n",
    "    else:\n",
    "        if len(posi)>len(nega):\n",
    "            senti[\"positive\"]+=1\n",
    "        elif len(nega)>len(posi):\n",
    "            senti[\"negative\"]+=1\n",
    "        else:\n",
    "            senti[\"mixed\"]+=1\n",
    "             \n",
    "  tokens.append(tokensent)\n",
    "  sentiment.append(sentilist)\n",
    "  sentiscore_neg=senti[\"negative\"]\n",
    "  sentiscore_pos=senti[\"positive\"]\n",
    "  sentiscore_mix=senti[\"mixed\"]\n",
    "  aggregated=senti[\"mixed\"]+senti[\"positive\"]-senti[\"negative\"]\n",
    "  sentiscore=aggregated/len(list(sentences))\n",
    "  sentiment_score_neg.append(sentiscore_neg)\n",
    "  sentiment_score_pos.append(sentiscore_pos)\n",
    "  sentiment_score_mix.append(sentiscore_mix)\n",
    "  sentiment_score.append(sentiscore)\n",
    "it_df_factor[\"lookup_sentiment\"]=sentiment\n",
    "it_df_factor[\"tokens\"]=tokens\n",
    "it_df_factor[\"sentiment_score_pos\"]=sentiment_score_pos\n",
    "it_df_factor[\"sentiment_score_neg\"]=sentiment_score_neg\n",
    "it_df_factor[\"sentiment_score_mixed\"]=sentiment_score_mix\n",
    "it_df_factor[\"sentiment_score\"]=sentiment_score\n",
    "it_df_factor.to_pickle('data/it_df_sentiments_lexicon2_no_dup.pkl')\n",
    "\n",
    "print(it_df_factor.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('data/it_df_sentiments_lexicon2_no_dup.pkl')\n",
    "sent_stats=df[\"sentiment_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f399e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_stats)\n",
    "print(min(sent_stats))\n",
    "print(max(sent_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=sent_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=sent_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.025,0.025])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos=0.4\n",
    "threshold_neg=0.7\n",
    "df_pos=sent_stats[(sent_stats>=threshold_pos)]\n",
    "df_neg=sent_stats[(sent_stats<=-threshold_neg)]\n",
    "print(len(df_neg))\n",
    "print(len(df_pos))\n",
    "sentiments=[]\n",
    "for s in sent_stats:\n",
    "    if s<=-threshold_neg:\n",
    "        sentiments.append(\"negative\")\n",
    "    elif s>=threshold_pos:\n",
    "        sentiments.append(\"positive\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "        \n",
    "df[\"lex2_sentiment\"]=sentiments\n",
    "df.to_pickle('data/it_df_sentiments_lexicon2_no_dup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score_neg\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(it_df_factor[\"sentiment_score_pos\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual=[]\n",
    "\n",
    "sentences=[]\n",
    "for i in tqdm(range(len(it_df_factor)), position=0, leave=True):\n",
    "  if visual:\n",
    "        sentences.append(visual)\n",
    "  visual=[]\n",
    "  for t in range(len(it_df_factor.loc[i,\"tokens\"])):\n",
    "    if it_df_factor.loc[i,\"lookup_sentiment\"][t]==\"negative\":\n",
    "      # adding markup commands to make background for negative sentiment crimson red\n",
    "      visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+it_df_factor.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    elif it_df_factor.loc[i,\"lookup_sentiment\"][t]==\"positive\":\n",
    "      # adding markup commands to make background for positive sentiment mediumseagreen\n",
    "      visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+it_df_factor.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    else:\n",
    "      visual.append(\" \"+it_df_factor.loc[i,\"tokens\"][t]+\" \")\n",
    "\n",
    "import markdown\n",
    "\n",
    "html_export=[]\n",
    "import IPython\n",
    "# helper function to render highlights in this notebook\n",
    "Markdown = lambda string: IPython.display.HTML(markdown.markdown(string))\n",
    "\n",
    "for s in sentences:\n",
    "    # display it in this notebokk\n",
    "    #display(Markdown(\" \".join(s)))\n",
    "    # prepare for export to html file\n",
    "    html = markdown.markdown(\" \".join(s))\n",
    "    html_export.append(html)\n",
    "\n",
    "# export to current working directory\n",
    "with open(\"./it_sentiments2.html\",\"w\", encoding=\"UTF-8\") as f:\n",
    "    for ht in html_export:\n",
    "        f.write(ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_df=it_df_factor[(it_df_factor[\"sentiment_score_neg\"] >= 30)|(it_df_factor[\"sentiment_score_pos\"]>=20)]\n",
    "extreme_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "visual=[]\n",
    "\n",
    "sentences=[]\n",
    "for i in tqdm(range(len(extreme_df[:10])), position=0, leave=True):\n",
    "  if visual:\n",
    "        sentences.append(visual)\n",
    "  visual=[]\n",
    "  for t in range(len(extreme_df.loc[i,\"tokens\"])):\n",
    "    if extreme_df.loc[i,\"lookup_sentiment\"][t]==\"negative\":\n",
    "      # adding markup commands to make background for negative sentiment crimson red\n",
    "      visual.append(str(\"<code style=\"+'\"background:crimson;color:white\"'+\"> \"+extreme_df.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    elif extreme_df.loc[i,\"lookup_sentiment\"][t]==\"positive\":\n",
    "      # adding markup commands to make background for positive sentiment mediumseagreen\n",
    "      visual.append(str(\"<code style=\"+'\"background:mediumseagreen;color:white\"'+\"> \"+extreme_df.loc[i,\"tokens\"][t]+\" </code>\"))\n",
    "    else:\n",
    "      visual.append(\" \"+extreme_df.loc[i,\"tokens\"][t]+\" \")\n",
    "\n",
    "import markdown\n",
    "\n",
    "\n",
    "import IPython\n",
    "# helper function to render highlights in this notebook\n",
    "Markdown = lambda string: IPython.display.HTML(markdown.markdown(string))\n",
    "# for instance based inspection use row index and score\n",
    "for s in sentences:\n",
    "    # display it in this notebokk\n",
    "    display(Markdown(\" \".join(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf27299",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "bert_df=pd.read_pickle(\"data/it_df_sentiments_bert_whole_len_no_dup.pkl\")\n",
    "sentita_df=pd.read_pickle('data/it_df_sentiments_sentita_whole_len_no_dup.pkl')\n",
    "lex1_df=pd.read_pickle('data/it_df_sentiments_lexicon1_no_dup.pkl')\n",
    "lex2_df=pd.read_pickle('data/it_df_sentiments_lexicon2_no_dup.pkl')\n",
    "\n",
    "print(\"bert:\")\n",
    "print(bert_df.groupby('sentiment').count()['id'])\n",
    "print(\"sentita:\")\n",
    "print(sentita_df.groupby('sentiment').count()['id'])\n",
    "print(\"lex1 (ragusa):\")\n",
    "print(lex1_df.groupby('lex1_sentiment').count()['id'])\n",
    "print(\"lex2 (porcu):\")\n",
    "print(lex2_df.groupby('lex2_sentiment').count()['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(\"Pairwise Cohen Kappa:\")\n",
    "print(str(\"BERT and Sentita: \"+str(cohen_kappa_score(bert_df[\"sentiment\"],sentita_df[\"sentiment\"]))))\n",
    "print(str(\"BERT and Lex1: \"+str(cohen_kappa_score(bert_df[\"sentiment\"],lex1_df[\"lex1_sentiment\"]))))\n",
    "print(str(\"BERT and Lex2: \"+str(cohen_kappa_score(bert_df[\"sentiment\"],lex2_df[\"lex2_sentiment\"]))))\n",
    "print(str(\"Sentita and Lex1: \"+str(cohen_kappa_score(sentita_df[\"sentiment\"],lex1_df[\"lex1_sentiment\"]))))\n",
    "print(str(\"Sentita and Lex2: \"+str(cohen_kappa_score(sentita_df[\"sentiment\"],lex2_df[\"lex2_sentiment\"]))))\n",
    "print(str(\"Lex1 and Lex2: \"+str(cohen_kappa_score(lex1_df[\"lex1_sentiment\"],lex2_df[\"lex2_sentiment\"]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fleiss kappa\n",
    "from nltk import agreement\n",
    "\n",
    "s1=bert_df[\"sentiment\"].tolist()\n",
    "s2=sentita_df[\"sentiment\"].tolist()\n",
    "s3=lex1_df[\"lex1_sentiment\"].tolist()\n",
    "s4=lex2_df[\"lex2_sentiment\"].tolist()\n",
    "\n",
    "formatted_codes = [[1,i,s1[i]] for i in range(len(s1))] + [[2,i,s2[i]] for i in range(len(s2))]  + [[3,i,s3[i]] for i in range(len(s3))]+ [[4,i,s4[i]] for i in range(len(s4))]\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_codes)\n",
    "print('Fleiss\\'s Kappa:',ratingtask.multi_kappa())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
