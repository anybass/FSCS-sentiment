{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e014bb4",
   "metadata": {},
   "source": [
    "## German-Sentiment (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf91f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install germansentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613448ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "model = SentimentModel()\n",
    "sentiment=[]\n",
    "de_df_factor = pd.read_pickle('data/de_df_factor.pkl')\n",
    "de_df_factor=de_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "pos=[]\n",
    "neu=[]\n",
    "neg=[]\n",
    "for l in range(len(de_df_factor)):\n",
    "    pos.append(0)\n",
    "    neu.append(0)\n",
    "    neg.append(0)\n",
    "    sentiment.append(0)\n",
    "for i in tqdm(range(len(de_df_factor)), position=0, leave=True):\n",
    "\n",
    "    sentiment[i], probabilities= model.predict_sentiment([de_df_factor.text[i]], output_probabilities = True)\n",
    "\n",
    "\n",
    "    for p in probabilities[0]:\n",
    "        if p[0]== \"neutral\":\n",
    "            neu[i]=p[1]\n",
    "        elif p[0]==\"positive\":\n",
    "            pos[i]=p[1]\n",
    "        elif p[0]==\"negative\":\n",
    "            neg[i]=p[1]\n",
    "                \n",
    "        \n",
    "    \n",
    "\n",
    "de_df_factor[\"sentiment\"]=sentiment\n",
    "de_df_factor[\"p_pos\"]=pos\n",
    "de_df_factor[\"p_neu\"]=neu\n",
    "de_df_factor[\"p_neg\"]=neg\n",
    "\n",
    "de_df_factor.to_pickle('data/de_df_sentiments_bert_whole_len_no_dup.pkl')\n",
    "de_df_factor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df_bert=pd.read_pickle('data/de_df_sentiments_bert_whole_len_no_dup.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25160f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df_bert.groupby('sentiment').count()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f89e4f",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob\n",
    "de_df_factor=pd.read_pickle('data/de_df_sentiments_no_dup.pkl')\n",
    "\n",
    "de_df_factor=de_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "\n",
    "class_dict=defaultdict(int)\n",
    "subjectivity_dict=defaultdict(int)\n",
    "sentiment=[]\n",
    "subjectivity=[]\n",
    "sent_stats=[]\n",
    "subj_stats=[]\n",
    "for i in tqdm(range(len(de_df_factor)), position=0, leave=True):\n",
    "    \n",
    "    classes = TextBlob(de_df_factor.text[i])\n",
    "    sent=classes.sentiment.polarity\n",
    "    sent_stats.append(sent)\n",
    "  \n",
    "    if sent<=-0.35:\n",
    "        class_dict[\"negative\"]+=1\n",
    "        sentiment.append(\"negative\")\n",
    "\n",
    "    elif sent>=0.4:\n",
    "        class_dict[\"positive\"]+=1\n",
    "        sentiment.append(\"positive\")\n",
    "      \n",
    "\n",
    "    else:\n",
    "        class_dict[\"neutral\"]+=1 \n",
    "        sentiment.append(\"neutral\")\n",
    "\n",
    "    sub=classes.sentiment.subjectivity\n",
    "    subj_stats.append(sub)\n",
    "\n",
    "\n",
    "    if sub>=0.01:\n",
    "        subjectivity_dict[\"subjective\"]+=1\n",
    "        subjectivity.append(\"subjective\")\n",
    "    \n",
    "    else:\n",
    "        subjectivity_dict[\"not_subjective\"]+=1 \n",
    "        subjectivity.append(\"not_subjective\")\n",
    "\n",
    "      \n",
    "    \n",
    "de_df_factor[\"sentiment\"]=sentiment\n",
    "de_df_factor[\"sentiment_score\"]=sent_stats\n",
    "de_df_factor[\"subjectivity\"]=subjectivity\n",
    "de_df_factor[\"subjectivity_score\"]=subj_stats\n",
    "de_df_factor.to_pickle('data/de_df_sentiments_no_dup_3classes.pkl')\n",
    "print(class_dict.items())\n",
    "print(subjectivity_dict.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_stats)\n",
    "print(min(sent_stats))\n",
    "print(max(sent_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=sent_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=sent_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.35,0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subj_stats)\n",
    "print(min(subj_stats))\n",
    "print(max(subj_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=subj_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=subj_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([0,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47508531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree\n",
    "def load(path):\n",
    "        \"\"\" Loads the XML-file (with sentiment annotations) from the given path.\n",
    "            By default, Sentiment.path is lazily loaded.\n",
    "        \"\"\"\n",
    "        # <word form=\"great\" wordnet_id=\"a-01123879\" pos=\"JJ\" polarity=\"1.0\" subjectivity=\"1.0\" intensity=\"1.0\" />\n",
    "        # <word form=\"damnmit\" polarity=\"-0.75\" subjectivity=\"1.0\" label=\"profanity\" />\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        words, synsets, labels = {}, {}, {}\n",
    "        xml = cElementTree.parse(path)\n",
    "        xml = xml.getroot()\n",
    "        lex={}\n",
    "        for w in xml.findall(\"word\"):\n",
    "            \n",
    "                w, s  = (\n",
    "                    w.attrib.get(\"form\"),\n",
    "                    w.attrib.get(\"subjectivity\", 0.0),\n",
    "                    \n",
    "                )\n",
    "                if float(s)>0.0:\n",
    "                    lex[w]=s\n",
    "        \n",
    "        return lex     \n",
    "lookup=load(\"data/de-sentiment.xml\")\n",
    "print(lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7996903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "subj_df=textblob_df[(textblob_df[\"subjectivity_score\"]>=0.1)]\n",
    "subj_df.reset_index(drop=True, inplace=True)\n",
    "print(len(subj_df))\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "subjsent=[]\n",
    "for i in range(len(subj_df)):\n",
    "    sentences=sent_tokenize(subj_df.loc[i].text, language=\"german\")\n",
    "    \n",
    "    for s in sentences:\n",
    "        token=word_tokenize(s, language=\"german\")\n",
    "        \n",
    "        for tok in token:\n",
    "            if tok.lower() in lookup.keys():\n",
    "                  subjsent.append(s)\n",
    "\n",
    "subs=\" \".join(list(subjsent))\n",
    "text = subs\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "d = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "nichtinteressant = \"e es war H vor zum keiner sowie aufgrund Oktober Juni dem hatte zur체ck dass April September stellte diese einem eines wegen seine diese November sie einer 체ber B da einen auch sich als er ein ihm seien X M채rz Dezember Februar IV zur welche weshalb wie bei habe weil ihr aus dieser nach dem Januar Mai Juli sei Nachdem August und von Der eine vom ab nicht mit mehr Fr des zu f체r seit viel keine bis das den wir ist die auf im\"\n",
    "liste_der_unerwuenschten_woerter = nichtinteressant.split()\n",
    "\n",
    "STOPWORDS.update(liste_der_unerwuenschten_woerter)\n",
    "wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(subjsent)\n",
    "print(len(subjsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "textblob_df=pd.read_pickle('data/de_df_sentiments_no_dup_3classes.pkl')\n",
    "from collections import defaultdict\n",
    "textblob_dict=defaultdict(int)\n",
    "t_list=[]\n",
    "for t in textblob_df[\"sentiment\"]:\n",
    "    textblob_dict[t]+=1\n",
    "textblob_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "textblob_df=pd.read_pickle('data/de_df_sentiments_no_dup_3classes.pkl')\n",
    "from collections import defaultdict\n",
    "textblob_dict=defaultdict(int)\n",
    "t_list=[]\n",
    "for t in textblob_df[\"subjectivity\"]:\n",
    "    textblob_dict[t]+=1\n",
    "textblob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3039bf",
   "metadata": {},
   "source": [
    "## GerVADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GerVADER.vaderSentimentGER import SentimentIntensityAnalyzer\n",
    "\n",
    "#standard values in vader are:\n",
    "    #positive sentiment: compound score >= 0.05\n",
    "    #neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    #negative sentiment: compound score <= -0.05\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "de_nlp = spacy.load('de_core_news_lg')\n",
    "config = {\"punct_chars\": None}\n",
    "de_nlp.add_pipe(\"sentencizer\", config=config)\n",
    "sentiment=[]\n",
    "#only 3 classes to compare with other classifiers later\n",
    "gervader=[]\n",
    "n=[]\n",
    "p=[]\n",
    "neu=[]\n",
    "vader_scores=[]\n",
    "de_df_factor = pd.read_pickle('data/de_df_factor.pkl')\n",
    "de_df_factor=de_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "for i in tqdm(range(len(de_df_factor)), position=0, leave=True):\n",
    "    senti=defaultdict(int)\n",
    "    doc=de_nlp(de_df_factor.text[i])\n",
    "    senti[\"positive\"]=0\n",
    "    senti[\"negative\"]=0\n",
    "    senti[\"neutral\"]=0\n",
    "    for sentence in doc.sents:\n",
    "        sents=[]\n",
    "        vs=analyzer.polarity_scores(sentence.text)\n",
    "        \n",
    "        sent=vs[\"compound\"]\n",
    "        vader_scores.append(sent)\n",
    "        #we decided to not be so granular and only take negative, neutral and positive\n",
    "        \"\"\"\n",
    "        if sent>=0.5:\n",
    "            senti[\"strong_positive\"]+=1\n",
    "        elif sent<=-0.5:\n",
    "            senti[\"strong_negative\"]+=1\n",
    "        elif sent>-0.5 and sent<=-0.3:\n",
    "            senti[\"negative\"]+=1\n",
    "\n",
    "        elif sent<0.5 and sent>=0.3:     \n",
    "            senti[\"positive\"]+=1\n",
    "\n",
    "        elif sent>-0.3 and sent<=-0.1:\n",
    "            senti[\"slightly_negative\"]+=1\n",
    "\n",
    "        elif sent<0.3 and sent>=0.1:\n",
    "            senti[\"slightly_positive\"]+=1\n",
    "        \"\"\"\n",
    "        if sent>=0.4:\n",
    "            senti[\"positive\"]+=1\n",
    "        elif sent<=-0.4:\n",
    "            senti[\"negative\"]+=1\n",
    "        else:\n",
    "            senti[\"neutral\"]+=1\n",
    "\n",
    "    final_class=max(senti, key=senti.get)\n",
    "\n",
    "    p.append(senti[\"positive\"]/len(list(doc.sents)))\n",
    "    n.append(senti[\"negative\"]/len(list(doc.sents)))\n",
    "    neu.append(senti[\"neutral\"]/len(list(doc.sents)))\n",
    "    gervader.append(final_class)\n",
    "\n",
    "de_df_factor[\"gervader_sentiment\"]=gervader\n",
    "de_df_factor[\"positive\"]=p\n",
    "de_df_factor[\"negative\"]=n\n",
    "de_df_factor[\"neutral\"]=neu\n",
    "\n",
    "de_df_factor.to_pickle('data/de_df_sentiments_gervader_no_dup_3classes04.pkl')\n",
    "de_df_factor.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bfd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gervader_df=pd.read_pickle('data/de_df_sentiments_gervader_no_dup_3classes04.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00230673",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vader_scores)\n",
    "print(min(vader_scores))\n",
    "print(max(vader_scores))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=vader_scores)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=vader_scores,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.3,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cb74d",
   "metadata": {},
   "source": [
    "## SentiWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "de_nlp = spacy.load('de_core_news_lg')\n",
    "de_nlp.add_pipe('sentiws', config={'sentiws_path': 'data/sentiws/'})\n",
    "config = {\"punct_chars\": None}\n",
    "de_nlp.add_pipe(\"sentencizer\", config=config)\n",
    "sentiment=[]\n",
    "\n",
    "n=[]\n",
    "p=[]\n",
    "\n",
    "neu=[]\n",
    "sent_stats=[]\n",
    "de_df_factor = pd.read_pickle('data/de_df_factor.pkl')\n",
    "de_df_factor=de_df_factor.drop_duplicates(subset=['year',\"text\",\"label\",\"region\",\"canton\",\"category\",\"language\"], ignore_index=True)\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "for i in tqdm(range(len(de_df_factor)), position=0, leave=True):\n",
    "    senti=defaultdict(int)\n",
    "    doc=de_nlp(de_df_factor.text[i])\n",
    "    \n",
    "    senti[\"positive\"]=0\n",
    "    senti[\"negative\"]=0\n",
    "    senti[\"neutral\"]=0\n",
    "    for sentence in doc.sents:\n",
    "        sents=[]\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token._.sentiws:\n",
    "                sents.append(token._.sentiws)\n",
    "            else:\n",
    "                sents.append(0)\n",
    "        sent=mean(sents)\n",
    "        sent_stats.append(sent)\n",
    "     \n",
    "        if sent<-0.01:\n",
    "            senti[\"negative\"]+=1\n",
    "\n",
    "        elif sent>0.01:     \n",
    "            senti[\"positive\"]+=1\n",
    "\n",
    "        else:\n",
    "            senti[\"neutral\"]+=1\n",
    "    sentiment.append(max(senti, key=senti.get))\n",
    "\n",
    "    p.append(senti[\"positive\"]/len(list(doc.sents)))\n",
    "    n.append(senti[\"negative\"]/len(list(doc.sents)))\n",
    "    neu.append(senti[\"neutral\"]/len(list(doc.sents)))\n",
    "\n",
    "   \n",
    "de_df_factor[\"sentiws_sentiment\"]=sentiment\n",
    "\n",
    "de_df_factor[\"positive\"]=p\n",
    "de_df_factor[\"negative\"]=n\n",
    "\n",
    "de_df_factor[\"neutral\"]=neu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "de_df_factor.to_pickle('data/de_df_sentiments_sentiws_no_dup_new.pkl')\n",
    "de_df_factor.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_stats)\n",
    "print(min(sent_stats))\n",
    "print(max(sent_stats))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.violinplot(x=sent_stats)\n",
    "ax2 = plt.axes([0.2, 0.6, .2, .2], facecolor='y')\n",
    "sns.violinplot(x=sent_stats,ax=ax2)\n",
    "ax2.set_title('zoom')\n",
    "ax2.set_xlim([-0.025,0.025])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44150f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiws_df = pd.read_pickle('data/de_df_sentiments_sentiws_no_dup_new.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f621f7",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(\"Pairwise Cohen Kappa:\")\n",
    "print(str(\"SentiWS and GerVADER: \"+str(cohen_kappa_score(sentiws_df[\"sentiws_sentiment\"],gervader_df[\"gervader_sentiment\"]))))\n",
    "print(str(\"SentiWS and TextBlob: \"+str(cohen_kappa_score(sentiws_df[\"sentiws_sentiment\"],textblob_df[\"textblob_sentiment\"]))))\n",
    "print(str(\"SentiWS and BERT: \"+str(cohen_kappa_score(sentiws_df[\"sentiws_sentiment\"],de_df_bert[\"sentiment\"]))))\n",
    "print(str(\"GerVADER and TextBlob: \"+str(cohen_kappa_score(gervader_df[\"gervader_sentiment\"],textblob_df[\"textblob_sentiment\"]))))\n",
    "print(str(\"GerVADER and BERT: \"+str(cohen_kappa_score(gervader_df[\"gervader_sentiment\"],de_df_bert[\"sentiment\"]))))\n",
    "print(str(\"TextBlob and BERT: \"+str(cohen_kappa_score(textblob_df[\"textblob_sentiment\"],de_df_bert[\"sentiment\"]))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fleiss kappa\n",
    "from nltk import agreement\n",
    "\n",
    "s1=sentiws_df[\"sentiws_sentiment\"].tolist()\n",
    "s2=gervader_df[\"gervader_sentiment\"].tolist()\n",
    "s3=textblob_df[\"textblob_sentiment\"].tolist()\n",
    "s4=de_df_bert[\"sentiment\"].tolist()\n",
    "\n",
    "formatted_codes = [[1,i,s1[i]] for i in range(len(s1))] + [[2,i,s2[i]] for i in range(len(s2))]  + [[3,i,s3[i]] for i in range(len(s3))]+ [[4,i,s4[i]] for i in range(len(s4))]\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_codes)\n",
    "print('Fleiss\\'s Kappa:',ratingtask.multi_kappa())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b23d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
